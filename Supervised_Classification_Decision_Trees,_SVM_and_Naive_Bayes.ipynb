{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "rqCC-wq5gnt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  \n",
        "Information Gain (IG) is a metric used to measure the effectiveness of an attribute in classifying data.  \n",
        "It is based on the concept of **entropy**, which measures the impurity or disorder of a dataset.\n",
        "\n",
        "The formula for Information Gain is:  \n",
        "\n",
        "\\`\\`\\`\n",
        "Information Gain = Entropy(Parent) - [Weighted Average] * Entropy(Children)\n",
        "\\`\\`\\`\n",
        "\n",
        "- A higher Information Gain means the attribute provides more information about the target class.\n",
        "- Decision Trees use IG to decide which feature to split on at each node — choosing the one with the **highest IG**.\n"
      ],
      "metadata": {
        "id": "TulNk_Nagr9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "oYxqowwTg1aO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Answer:**  \n",
        "| Metric | Formula | Range | Interpretation |\n",
        "|---------|----------|--------|----------------|\n",
        "| **Entropy** | \\(-p_1 \\log_2 p_1 - p_2 \\log_2 p_2 - ... - p_n \\log_2 p_n\\) | 0–1 | Measures impurity based on information theory |\n",
        "| **Gini Impurity** | \\(1 - \\sum p_i^2\\) | 0–0.5 | Measures how often a randomly chosen element is incorrectly labeled |\n",
        "\n",
        "**Key Differences:**\n",
        "- Entropy involves logarithmic computation (slower but information-theoretic).\n",
        "- Gini is simpler and faster to compute.\n",
        "- Both yield similar trees, but Gini often performs better in practice for large datasets.\n"
      ],
      "metadata": {
        "id": "Q5LK25BWg4-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "wmNvdze6hMLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  \n",
        "Pre-Pruning (also called **Early Stopping**) prevents a Decision Tree from becoming too complex.  \n",
        "Instead of fully growing the tree and pruning later, it stops the growth early when further splits don't significantly improve performance.\n",
        "\n",
        "**Common Pre-Pruning Criteria:**\n",
        "- Maximum depth (`max_depth`)\n",
        "- Minimum samples per leaf (`min_samples_leaf`)\n",
        "- Minimum information gain threshold\n",
        "\n",
        "This helps reduce **overfitting** and improves **generalization**.\n"
      ],
      "metadata": {
        "id": "0lblB60NhOX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "B4wJ4JmEhSJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Feature importances\n",
        "importances = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
        "print(\"Feature Importances:\")\n",
        "print(importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfM5zu5zhYJ8",
        "outputId": "16b72e91-8d69-4005-8e15-51d9c2c411dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "VEn4DfzHhbSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  \n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.  \n",
        "It finds the **optimal hyperplane** that best separates data points of different classes with **maximum margin**.\n",
        "\n",
        "Key concepts:\n",
        "- **Support Vectors:** Data points closest to the decision boundary.\n",
        "- **Margin:** Distance between the hyperplane and support vectors.\n",
        "- **Goal:** Maximize the margin for better generalization.\n"
      ],
      "metadata": {
        "id": "9XY3IkP-heXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?**Answer:**  \n"
      ],
      "metadata": {
        "id": "fv1eZVDrhh_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  \n",
        "The **Kernel Trick** allows SVM to perform nonlinear classification efficiently.  \n",
        "It maps data into a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "Common kernels:\n",
        "- **Linear:** Suitable for linearly separable data.\n",
        "- **Polynomial:** Adds interaction features.\n",
        "- **RBF (Radial Basis Function):** Handles complex, nonlinear boundaries.\n",
        "\n",
        "Formula example for RBF:  \n",
        "\\( K(x, x') = \\exp(-\\gamma ||x - x'||^2) \\)"
      ],
      "metadata": {
        "id": "927Q6Z-zhp8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "wAmxTP3whte_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy: {rbf_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQpeUcinhxSJ",
        "outputId": "6b1cdcc1-5ed4-4b3e-f61a-8bd98d75a978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0000\n",
            "RBF Kernel Accuracy: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called 'Naïve'?"
      ],
      "metadata": {
        "id": "7xfjzf5oh1Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**  \n",
        "Naïve Bayes is a **probabilistic classifier** based on **Bayes' Theorem**, assuming independence between features.\n",
        "\n",
        "Formula:  \n",
        "\\( P(C|X) = \\frac{P(X|C) * P(C)}{P(X)} \\)\n",
        "\n",
        "It’s called “Naïve” because it **assumes all features are independent**, which is rarely true in practice, but still performs surprisingly well for many problems.\n"
      ],
      "metadata": {
        "id": "qZzd-9g2h6ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
      ],
      "metadata": {
        "id": "vg-_XBo4h7yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Answer:**  \n",
        "| Type | Data Type | Use Case | Distribution |\n",
        "|------|------------|----------|---------------|\n",
        "| **GaussianNB** | Continuous | Real-valued features (e.g., Iris, Breast Cancer) | Normal distribution |\n",
        "| **MultinomialNB** | Discrete | Text classification (word counts) | Multinomial distribution |\n",
        "| **BernoulliNB** | Binary | Binary/boolean features (e.g., presence/absence of word) | Bernoulli distribution |"
      ],
      "metadata": {
        "id": "I3rFQZlLh_Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "RS88uNbViDNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"GaussianNB Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CBVfmcliKl6",
        "outputId": "6637e067-1def-4347-da4a-3e39c4d644e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GaussianNB Accuracy: 0.9737\n"
          ]
        }
      ]
    }
  ]
}